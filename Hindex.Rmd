---
title: "Honours methods"
author: "Jess"
date: "2/21/2020"
output: html_document
---

```{r, setup}
pacman::p_load(tidyverse,
               rscopus,
               httr,
               XML,
               taxize,
               rlang)
```

**Functions**
1. searchCount
  + counts the total number of search results on Scopus
2. *extractcontent
  + extracts content from Scopus using a search string
  + data is not ready for analysis; requires extractXML
3. *extractXML
  + converts data from extractcontent into a dataframe for analyses
4. FetchSpT
  + fetches data from Scopus using genus and species name
  + title only
5. FetchSpTAK
  + fetches data from Scopus using genus and species name
  + title + abstract + keywords
5.2 ^FetchSpTAKloop
  + loop added
6. TotalPub
  + counts the total number of publications
7. TotalCite
  + counts the total number of citations
8. TotalJournals
  + counts the total number of journals
9. TotalArt
  + counts the total number of articles
10. TotalRev
  + counts the total number of reviews
11. ARRatio
  + calculates article:review ratio
12. SpHindex
  + calculates h-index
13. YearsPublishing
  + calculates the number of years since the first publication
14. SpMindex
  + calculates m-index (h-index / total years)
15. Spi10
  + counts the publications with 10+ citations
16. SpH5
  + calculates h-index for the past 5 years
17. SpHAfterdate
  + calculates h-index with a given time frame
18. Allindices
  + shows a summary of the main indices

*function not available to users
^not functional yet



```{r, function to get result count}
#code from Fonti (modified)
searchCount <- function(genus, species, APIkey, datatype = "application/xml") {
  library(httr)
  library(XML)
  theURL <- GET("http://api.elsevier.com/content/search/scopus",
              query = list(apiKey = paste0(APIkey),
                           query = paste0("TITLE-ABS-KEY(\"",genus," ",species,"\") AND DOCTYPE(ar OR re)"),
                           httpAccept = "application/xml")) #format the URL to be sent to the API
  stop_for_status(theURL) #pass any HTTP errors to the R console
  theData <- content(theURL, as = "text") #extract the content of the response
  newData <- xmlParse(theURL) #parse the data to extract values
  resultCount <- as.numeric(xpathSApply(newData,"//opensearch:totalResults", xmlValue)) #get the total number of search results for the string
  return(resultCount)
}
```

```{r}
#code from Christopher Belter https://github.com/christopherBelter/scopusAPI/blob/master/scopusAPI.R
extractcontent<- function(search.string, datatype = "application/xml") {
  library(httr)
  library(XML)
  key <- "442b9048417ef20cf680a0ae26ee4d86" #my api key
  theURL <- GET("http://api.elsevier.com/content/search/scopus",
              query = list(apiKey = key,
                           query = paste(search.string),
                           httpAccept = "application/xml")) #format the URL to be sent to the API
  stop_for_status(theURL) #pass any HTTP errors to the R console
  theData <- content(theURL, as = "text") #extract the content of the response
  return(theData)
}
```

```{r, function to extract XML}
#code from Christopher Belter https://github.com/christopherBelter/scopusAPI/blob/master/scopusAPI.R
extractXML <- function(theFile) {
	##library(XML)
	newData <- XML::xmlParse(theFile) ## parse the XML
	records <- XML::getNodeSet(newData, "//cto:entry", namespaces = "cto") ## create a list of records for missing or duplicate node handling
	scopusID <- lapply(records, XML::xpathSApply, "./cto:eid", XML::xmlValue, namespaces = "cto") ## handle potentially missing eid nodes
	scopusID[sapply(scopusID, is.list)] <- NA
	scopusID <- unlist(scopusID)
	doi <- lapply(records, XML::xpathSApply, "./prism:doi", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/")) ## handle potentially missing doi nodes
	doi[sapply(doi, is.list)] <- NA
	doi <- unlist(doi)
	pmid <- lapply(records, XML::xpathSApply, "./cto:pubmed-id", XML::xmlValue, namespaces = "cto") ## handle potentially missing pmid nodes: returns a list with the node value if the node is present and an empty list if the node is missing
	pmid[sapply(pmid, is.list)] <- NA ## find the empty lists in pmid and set them to NA
	pmid <- unlist(pmid) ## turn the pmid list into a vector
	authLast <- lapply(records, XML::xpathSApply, ".//cto:surname", XML::xmlValue, namespaces = "cto") ## grab the surname and initials for each author in each record, then paste them together 
	authLast[sapply(authLast, is.list)] <- NA
	authInit <- lapply(records, XML::xpathSApply, ".//cto:initials", XML::xmlValue, namespaces = "cto")
	authInit[sapply(authInit, is.list)] <- NA
	authors <- mapply(paste, authLast, authInit, collapse = "|")
	authors <- sapply(strsplit(authors, "|", fixed = TRUE), unique) ## remove the duplicate author listings
	authors <- sapply(authors, paste, collapse = "|")
	affiliations <- lapply(records, XML::xpathSApply, ".//cto:affilname", XML::xmlValue, namespaces = "cto") ## handle multiple affiliation names
	affiliations[sapply(affiliations, is.list)] <- NA
	affiliations <- sapply(affiliations, paste, collapse = "|")
	affiliations <- sapply(strsplit(affiliations, "|", fixed = TRUE), unique) ## remove the duplicate affiliation listings
	affiliations <- sapply(affiliations, paste, collapse = "|")
	countries <- lapply(records, XML::xpathSApply, ".//cto:affiliation-country", XML::xmlValue, namespaces = "cto")
	countries[sapply(countries, is.list)] <- NA
	countries <- sapply(countries, paste, collapse = "|")
	countries <- sapply(strsplit(countries, "|", fixed = TRUE), unique) ## remove the duplicate country listings
	countries <- sapply(countries, paste, collapse = "|") 
	year <- lapply(records, XML::xpathSApply, "./prism:coverDate", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/"))
	year[sapply(year, is.list)] <- NA
	year <- unlist(year)
	year <- gsub("\\-..", "", year) ## extract only year from coverDate string (e.g. extract "2015" from "2015-01-01")
	articletitle <- lapply(records, XML::xpathSApply, "./dc:title", XML::xmlValue, namespaces = c(dc = "http://purl.org/dc/elements/1.1/"))
	articletitle[sapply(articletitle, is.list)] <- NA
	articletitle <- unlist(articletitle)
	journal <- lapply(records, XML::xpathSApply, "./prism:publicationName", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/")) ## handle potentially missing issue nodes
	journal[sapply(journal, is.list)] <- NA
	journal <- unlist(journal)
	volume <- lapply(records, XML::xpathSApply, "./prism:volume", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/")) ## handle potentially missing issue nodes
	volume[sapply(volume, is.list)] <- NA
	volume <- unlist(volume)
	issue <- lapply(records, XML::xpathSApply, "./prism:issueIdentifier", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/")) ## handle potentially missing issue nodes
	issue[sapply(issue, is.list)] <- NA
	issue <- unlist(issue)
	pages <- lapply(records, XML::xpathSApply, "./prism:pageRange", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/")) ## handle potentially missing issue nodes
	pages[sapply(pages, is.list)] <- NA
	pages <- unlist(pages)
	abstract <- lapply(records, XML::xpathSApply, "./dc:description", XML::xmlValue, namespaces = c(dc = "http://purl.org/dc/elements/1.1/")) ## handle potentially missing abstract nodes
	abstract[sapply(abstract, is.list)] <- NA
	abstract <- unlist(abstract)
	keywords <- lapply(records, XML::xpathSApply, "./cto:authkeywords", XML::xmlValue, namespaces = "cto")
	keywords[sapply(keywords, is.list)] <- NA
	keywords <- unlist(keywords)
	keywords <- gsub(" | ", "|", keywords, fixed = TRUE)
	ptype <- lapply(records, XML::xpathSApply, "./cto:subtypeDescription", XML::xmlValue, namespaces = "cto")
	ptype[sapply(ptype, is.list)] <- NA
	ptype <- unlist(ptype)
	timescited <- lapply(records, XML::xpathSApply, "./cto:citedby-count", XML::xmlValue, namespaces = "cto")
	timescited[sapply(timescited, is.list)] <- NA
	timescited <- unlist(timescited)
	theDF <- data.frame(scopusID, doi, pmid, authors, affiliations, countries, year, articletitle, journal, volume, issue, pages, keywords, abstract, ptype, timescited, stringsAsFactors = FALSE)
	return(theDF)
}
```

```{r, fetch using title only}
#this function downloads citation data from scopus
FetchSpT <- function(genus, species, APIkey) {
  library(rscopus)
  library(rlang)
  library(dplyr)
  if (is_missing(APIkey)) {
    stop("You need to register for an API key on Scopus.") #stops the function from running
  }
  count <- searchCount(genus, species, APIkey)
  print(paste(count, "records found."))
  step_size <- 1000
  start_record <- 0
  datalist = list()
  looprepeat <- ceiling(count/step_size)
  #loop starts
  for (i in 1:looprepeat) { 
    print(paste("starting iteration: ", i, " Note: iteration size is ", step_size, " records, which runs of 200 records inside each iteration."))
    print(paste("Fetching records now."))
    search <- scopus_search(query = paste0("TITLE(\"",genus," ",species,"\") AND DOCTYPE(ar OR re)"),
                          api_key = paste0(APIkey),
                          verbose = TRUE,
                          max_count = step_size,
                          start = start_record,
                          wait_time = 1)
    start_record <- as.numeric(summary(search)[1,1]) #move the pointer of starting record for each iteration to a new value
    searchdf <- entries_to_citation_df(search$entries)
    datalist[[i]] <- searchdf
    print(paste("Retrieved", start_record, "records."))
  }
  #loop ends
  searchcombine <- do.call(rbind, datalist) # convert list of dataframes into one big dataframe
  returned <- dim(searchcombine)[1]
  print(paste(returned, "records retrived in total."))
  #remove duplicates
  duplicates <- dim(searchcombine[duplicated(searchcombine$title),])[1]
  print(paste(duplicates, "duplicates found."))
  if (duplicates>0) {
    print(paste("Removing duplicated records."))
    searchcombine <- searchcombine[!duplicated(searchcombine$title), ] 
  }
  #showing final list of records
  retrieved <- dim(searchcombine)[1] #check the number
  print(paste(retrieved, "unique records successfully fetched."))
  return(searchcombine)
}
```

```{r, fetch using title+abs+key}
#this function downloads citation data from scopus
FetchSpTAK <- function(genus, species, APIkey) {
  library(rscopus)
  library(rlang)
  library(dplyr)
  if (is_missing(APIkey)) {
    stop("You need to register for an API key on Scopus.") #stops the function from running
  }
  count <- searchCount(genus, species, APIkey)
  print(paste(count, "records found."))
  step_size <- 1000
  start_record <- 0
  datalist = list()
  looprepeat <- ceiling(count/step_size)
  #loop starts
  for (i in 1:looprepeat) { 
    print(paste("starting iteration: ", i, " Note: iteration size is ", step_size, " records, which runs of 200 records inside each iteration."))
    print(paste("Fetching records now."))
    search <- scopus_search(query = paste0("TITLE-ABS-KEY(\"",genus," ",species,"\") AND DOCTYPE(ar OR re)"),
                          api_key = paste0(APIkey),
                          verbose = TRUE,
                          max_count = step_size,
                          start = start_record,
                          wait_time = 1)
    start_record <- as.numeric(summary(search)[1,1]) #move the pointer of starting record for each iteration to a new value
    searchdf <- entries_to_citation_df(search$entries)
    datalist[[i]] <- searchdf
    print(paste("Retrieved", start_record, "records."))
  }
  #loop ends
  searchcombine <- do.call(rbind, datalist) # convert list of dataframes into one big dataframe
  returned <- dim(searchcombine)[1]
  print(paste(returned, "records retrived in total."))
  #remove duplicates
  duplicates <- dim(searchcombine[duplicated(searchcombine$title),])[1]
  print(paste(duplicates, "duplicates found."))
  if (duplicates>0) {
    print(paste("Removing duplicated records."))
    searchcombine <- searchcombine[!duplicated(searchcombine$title), ] 
  }
  #showing final list of records
  retrieved <- dim(searchcombine)[1] #check the number
  print(paste(retrieved, "unique records successfully fetched."))
  return(searchcombine)
}
```

```{r, total publications}
#this function calculates the total number of publications
TotalPub <- function(data) {
  total <- nrow(data) #counts the number of publications by summing the records
  return(total)
}
```

```{r, total citations}
#this function calculates the total number of citations
TotalCite <- function(data) {
  data$citations <- as.numeric(data$citations) #changes the class to numerical for calculation
  total <- sum(data$citations) #sums all of the citations
  return(total)
}
```

```{r, total journals}
#this function calculates the total number of journals
TotalJournals <- function(data) {
  filter <- unique(data$journal) #filters data to pick out each journal
  total <- length(filter) #counts the number of journals
  return(total)
}
```

```{r, total articles}
#this function calculates the total number of articles
TotalArt <- function(data) {
  Article <- sum(data$description == "Article") #calculates the number of articles
  return(Article)
}
```

```{r, total reviews}
#this function calculates the total number of reviews
TotalRev <- function(data) {
  Review <- sum(data$description == "Review") #calculates the number of reviews
  return(Review)
}
```

```{r, article:review ratio}
#this function calculates the ratio of articles:rerviews
ARRatio <- function(data) {
  Article <- sum(data$description == "Article") #calculates the number of articles
  Review <- sum(data$description == "Review") #calculates the number of reviews
  ArticleRatio <- Article/(Article+Review)*100 #percentage of articles
  ReviewRatio <- Review/(Article+Review)*100 #percentage of reviews
  Ratio <- paste(ArticleRatio, ":", ReviewRatio) #prints ratio
  return(Ratio)
}
```

```{r, species h-index}
#this function calculates the h-index
SpHindex <- function(data) {
  data$citations <- as.numeric(data$citations) #change class from factor to numeric
  sorteddf <- sort(data$citations, decreasing = TRUE) #sort in descending order
  Hindex <- 0   #computes h-index
  for(i in 1:length(sorteddf)) {
    if (sorteddf[i] > Hindex) {
      Hindex <- Hindex + 1
    }
  }
  return(Hindex)
}
```

```{r, years of publication}
#this function canculates the number of years since the first publication
YearsPublishing <- function(data) {
  data$year <- as.numeric(substr(data$cover_date, 1, 4))
  as.numeric(substr(Sys.Date(), 1, 4)) - min(data$year) #time since first publication in years
  years_publishing <- as.numeric(substr(Sys.Date(), 1, 4)) - min(data$year) #time since first publication in years
  return(years_publishing)
}
```

```{r, m-index}
#this function calculates the m-index
#it does not requires the calculation of the h-index beforehand
SpMindex <- function(data) {
  data$citations <- as.numeric(data$citations) #change class from factor to numeric
  sorteddf <- sort(data$citations, decreasing = TRUE) #sort in descending order
  Hindex <- 0   #computes h-index
  for(i in 1:length(sorteddf)) {
    if (sorteddf[i] > Hindex) {
      Hindex <- Hindex + 1
    }
  }
  data$year <- as.numeric(substr(data$cover_date, 1, 4))
  years_publishing <- as.numeric(substr(Sys.Date(), 1, 4)) - min(data$year) #time since first publication in years
  Mindex <- Hindex/years_publishing
  return(Mindex)
}
```

```{r, i10 index}
#this function calculates the i10 index
#i10 index counts all of the publications with 10+ citations
Spi10 <- function(data) {
  data$citations <- as.numeric(data$citations) #changes class from factor to numeric
  sorteddf <- sort(data$citations, decreasing = TRUE) #sorts in descending order
  i10 <- sum(sorteddf>=10) #counts the publications with 10 or more citations
  return(i10)
}
```

```{r, h5 index}
#this function calculates the h-index of the past 5 years
SpH5 <- function(data) { #last 5 years from the current date
  current_date <- as.numeric(substr(Sys.Date(), 1, 4)) #current year
  #The easiest thing to do is to convert it into POSIXlt and subtract 5 from the years slot.
  d <- as.POSIXlt(Sys.Date())
  d$year <- d$year-5
  as.Date(d)
  return(HAfterdate(data, d))
}
```

```{r, h-index with a given date}
#this function calculates the h-index using a given date up till the newest record
SpHAfterdate <- function(data, date) {
  library(dplyr)
  data$cover_date <- as.Date(data$cover_date, format = "%Y-%m-%d") #change format of the date from factor to date
  subsetdata <- filter(data, cover_date > as.Date(date) )
  HAfterdate <- Hindex(subsetdata)
  return(HAfterdate)
}
```

```{r, shows main indices}
#this function returns a summary of all of the indices
Allindices <- function(data) {
  combine <- data.frame(TotalPub(data), TotalCite(data), TotalJournals(data), TotalArt(data),
                        TotalRev(data), YearsPublishing(data), SpHindex(data), SpMindex(data), Spi10(data),
                        SpH5(data))
  colnames(combine) <- c("publications", "citations", "journals", "articles",
                         "reviews", "years_publishing", "h", "m", "i10",
                         "h5")
  cat("", TotalPub(data), "publications", "\n",
      TotalCite(data), "citations", "\n",
      TotalJournals(data), "journals", "\n",
      TotalArt(data), "articles", "\n",
      TotalRev(data), "reviews", "\n",
      YearsPublishing(data), "years of publishing", "\n",
      "h:", SpHindex(data), "\n",
      "m:", SpMindex(data), "\n",
      "i10:", Spi10(data), "\n",
      "h5:", SpH5(data))
  return(combine)
}
```

**Tests**

```{r, woylie}
#test done on 10 july
searchCount("Bettongia", "penicillata", "442b9048417ef20cf680a0ae26ee4d86") #113
Woylie <- FetchSpTAK("Bettongia", "penicillata", "442b9048417ef20cf680a0ae26ee4d86") #no duplicates

TotalPub(Woylie) #113
TotalCite(Woylie) #1897
TotalJournals(Woylie) #55
TotalArt(Woylie) #110
TotalRev(Woylie) #3
ARRatio(Woylie) #97.3451327433628:2.65486725663717

YearsPublishing(Woylie) #43
SpHindex(Woylie) #26
SpMindex(Woylie) #0.6046512
Spi10(Woylie) #54
SpH5(Woylie) #8
SpHAfterdate(Woylie, "2000-01-01") #20

B_penicillata <- Allindices(Woylie)
```

```{r, quokka}
#test done on 10 july
searchCount("Setonix", "brachyurus", "442b9048417ef20cf680a0ae26ee4d86") #242
Quokka <- FetchSpTAK("Setonix", "brachyurus", "442b9048417ef20cf680a0ae26ee4d86") #no duplicates

TotalPub(Quokka) #242
TotalCite(Quokka) #3423
TotalJournals(Quokka) #107
TotalArt(Quokka) #237
TotalRev(Quokka) #5
ARRatio(Quokka) #97.9338842975207:2.06611570247934

YearsPublishing(Quokka) #66
SpHindex(Quokka) #29
SpMindex(Quokka) #0.4393939
Spi10(Quokka) #121
SpH5(Quokka) #3
SpHAfterdate(Quokka, "2000-01-01") #17

S_brachyurus <- Allindices(Quokka)
```

```{r, platypus}
#test done on 10 july
searchCount("Ornithorhynchus", "anatinus", "442b9048417ef20cf680a0ae26ee4d86") #321
Platypus <- FetchSpTAK("Ornithorhynchus", "anatinus", "442b9048417ef20cf680a0ae26ee4d86") #no duplicates

TotalPub(Platypus) #321
TotalCite(Platypus) #6363
TotalJournals(Platypus) #153
TotalArt(Platypus) #308
TotalRev(Platypus) #13
ARRatio(Platypus) #95.9501557632399 : 4.04984423676012

YearsPublishing(Platypus) #67
SpHindex(Platypus) #41
SpMindex(Platypus) #0.6119403
Spi10(Platypus) #177
SpH5(Platypus) #7
SpHAfterdate(Platypus, "2000-01-01") #32

O_anatinus <- Allindices(Platypus)
```

```{r, koala}
#test done on 10 july
searchCount("Phascolarctos", "cinereus", "442b9048417ef20cf680a0ae26ee4d86") #777
#did we not have 930 records?? hmm...
Koala <- FetchSpTAK("Phascolarctos", "cinereus", "442b9048417ef20cf680a0ae26ee4d86") #4 duplicates

TotalPub(Koala) #773
TotalCite(Koala) #14291
TotalJournals(Koala) #227
TotalArt(Koala) #744
TotalRev(Koala) #29
ARRatio(Koala) #96.248382923674 : 3.751617076326

YearsPublishing(Koala) #139
SpHindex(Koala) #53
SpMindex(Koala) #0.381295
Spi10(Koala) #427
SpH5(Koala) #15
SpHAfterdate(Koala, "2000-01-01") #44

P_cinereus <- Allindices(Koala)
```

```{r, combining the datasets}
Combinedata <- rbind(B_penicillata, S_brachyurus, O_anatinus, P_cinereus)

plot(Combinedata$publications)
plot(Combinedata$citations)
plot(Combinedata$journals)
plot(Combinedata$articles)
plot(Combinedata$reviews)
plot(Combinedata$years_publishing)
plot(Combinedata$h)
plot(Combinedata$m) #this is quite interesting
plot(Combinedata$i10)
plot(Combinedata$h5) #woylie has a pretty high index here
```

```{r, manual tests}
#test with dummy data
dummy1 <- data.frame("citations" = c(12, 5, 0, 21, 3, 4, 9, 17, 1, 10,
                                     10, 1, 0, 16, 26, 24, 15, 34, 31, 29),
                     "cover_date" = c(2000, 2015, 1986, 1994, 1972, 2002, 2006, 2019, 2016, 2006,
                                      1997, 1989, 2007, 2010, 2013, 2020, 2017, 1994, 1999, 2016))
SpHindex(dummy1) #10
YearsPublishing(dummy1) #48
SpMindex(dummy1) #0.2083333
Spi10(dummy1) #12

```



```{r, rscopus}
#some tests
tiger <- FetchSpTAK("Panthera", "tigris", "442b9048417ef20cf680a0ae26ee4d86") #402 results
searchCount("Panthera", "tigris", "442b9048417ef20cf680a0ae26ee4d86")
dim(tiger[duplicated(tiger$title),])[1]

tiger <- FetchSpTAK("Panthera", "tigris") #shows warning
fox <- FetchSpT("Vulpes", "vulpes", "442b9048417ef20cf680a0ae26ee4d86") #1133 results

foxTAK <- FetchSpTAK("Vulpes", "vulpes", "442b9048417ef20cf680a0ae26ee4d86") #3324 results
drosophila <- FetchSpTAK("Drosophila", "montana", "442b9048417ef20cf680a0ae26ee4d86") #93 results
drosophila2 <- FetchSpTAK("drosophila", "melanogaster", "442b9048417ef20cf680a0ae26ee4d86") #60904 results, error at 23%
drosophila3 <- FetchSpTAK("Mus", "musculus", "442b9048417ef20cf680a0ae26ee4d86") 

cat <- FetchSpTAK("felis", "catus", "442b9048417ef20cf680a0ae26ee4d86") #17622 results, error at 26%, current loop can only output 2000 records

elephant <- FetchSpTAK("Loxodonta","africana", "442b9048417ef20cf680a0ae26ee4d86") #1401 results
dim(elephant[duplicated(elephant$title),])[1]

possum <- FetchSpTAKloop("Burramys", "parvus", "442b9048417ef20cf680a0ae26ee4d86") #45
dim(possum[duplicated(possum$title),])[1]

searchCount("Mus", "musculus", "442b9048417ef20cf680a0ae26ee4d86") #34252 count

Pub <- TotalPub(tiger) #402
Cite <- TotalCite(tiger) #5244
Journal <- TotalJournals(tiger) #168
Article <- TotalArt(tiger)
Review <- TotalRev(tiger)
Ratio <- ARRatio(tiger) #97.5124378109453 : 2.48756218905473
class(Article)

H <- SpHindex(tiger) #53
years <- YearsPublishing(tiger) #139
M <- SpMindex(tiger) #0.381294964028777
i10 <- Spi10(tiger) #467
h5 <- SpH5(tiger) #15
datexdf <- HAfterdate(xdf, "2000-01-01") #47
ALL <- Allindices(tiger)

```



```{r, Losia's h-index code}
      
citations <- c(0) #0
citations <- c(1) #1
citations <- c(0,0,0,0,0,0) #0
citations <- c(0,0,1,0,0,0) #1
citations <- c(2,3,0,5,6,0) #3
citations <- c(0,1,0,5,6,0) #2
citations <- c(4,4,4,1,4,0) #4
citations <- c(5,5,5,5,5,5) #5
citations <- c(6,6,6,6,6,6) #6
citations <- c(6,6,6,6,6,5) #5
citations <- c(6,6,6,9,9,9) #5
citations <- xdf$citations #53?

citations <- sort(citations, decreasing = TRUE)

mycount <- 0   
for(i in 1:length(citations)) {
  if (citations[i] > mycount) {
    mycount <- mycount + 1
  }
}
print(paste("h: ", mycount))

#see: https://publons.com/blog/5-things-the-h-index-cant-tell-you/
#also: m (Hirsch, 2005). M measures the slope or rate of increase of the H-index over time and is, in my view, a greatly underappreciated measure.   To calculate the m-value, take the researchers H-index and divide by the number of years since their first publication.
#H5Y factor. It is the H factor, but calculated only on citations received in the past five years. 

#i10 - check it out?
```
