---
title: "Honours methods"
author: "Jess"
date: "2/21/2020"
output: html_document
---

```{r, setup}
pacman::p_load(tidyverse,
               rscopus,
               httr,
               XML,
               taxize)
```

**Functions**
1. searchCount
  + counts the total number of search results on Scopus
2. *extractcontent
  + extracts content from Scopus using a search string
  + data is not ready for analysis; requires extractXML
3. *extractXML
  + converts data from extractcontent into a dataframe for analyses
4. FetchSpT
  + fetches data from Scopus using genus and species name
  + title only
5. FetchSpTAK
  + fetches data from Scopus using genus and species name
  + title + abstract + keywords
5.2 ^FetchSpTAKloop
  + loop added
6. TotalPub
  + counts the total number of publications
7. TotalCite
  + counts the total number of citations
8. TotalJournals
  + counts the total number of journals
9. Ratio
  + counts the total number of articles and reviews
  + calculates article:review ratio
10. Hindex
  + calculates h-index
11. YearsPublishing
  + calculates the number of years since the first publication
12. Mindex
  + calculates m-index (h-index / total years)
13. i10
  + counts the publications with 10+ citations
14. H5
  + calculates h-index for the past 5 years
15. HAfterdate
  + calculates h-index with a given time frame
16. Allindices
  + shows a summary of the main indices

*function not available to users
^not functional yet



```{r, function to get result count}
#code from Fonti (modified)
searchCount <- function(genus, species, APIkey, datatype = "application/xml") {
  library(httr)
  library(XML)
  theURL <- GET("http://api.elsevier.com/content/search/scopus",
              query = list(apiKey = paste0(APIkey),
                           query = paste0("TITLE-ABS-KEY(\"",genus," ",species,"\") AND DOCTYPE(ar OR re)"),
                           httpAccept = "application/xml")) #format the URL to be sent to the API
  stop_for_status(theURL) #pass any HTTP errors to the R console
  theData <- content(theURL, as = "text") #extract the content of the response
  newData <- xmlParse(theURL) #parse the data to extract values
  resultCount <- as.numeric(xpathSApply(newData,"//opensearch:totalResults", xmlValue)) #get the total number of search results for the string
  return(resultCount)
}
```

```{r}
#code from Christopher Belter https://github.com/christopherBelter/scopusAPI/blob/master/scopusAPI.R
extractcontent<- function(search.string, datatype = "application/xml") {
  library(httr)
  library(XML)
  key <- "442b9048417ef20cf680a0ae26ee4d86" #my api key
  theURL <- GET("http://api.elsevier.com/content/search/scopus",
              query = list(apiKey = key,
                           query = paste(search.string),
                           httpAccept = "application/xml")) #format the URL to be sent to the API
  stop_for_status(theURL) #pass any HTTP errors to the R console
  theData <- content(theURL, as = "text") #extract the content of the response
  return(theData)
}
```

```{r, function to extract XML}
#code from Christopher Belter https://github.com/christopherBelter/scopusAPI/blob/master/scopusAPI.R
extractXML <- function(theFile) {
	##library(XML)
	newData <- XML::xmlParse(theFile) ## parse the XML
	records <- XML::getNodeSet(newData, "//cto:entry", namespaces = "cto") ## create a list of records for missing or duplicate node handling
	scopusID <- lapply(records, XML::xpathSApply, "./cto:eid", XML::xmlValue, namespaces = "cto") ## handle potentially missing eid nodes
	scopusID[sapply(scopusID, is.list)] <- NA
	scopusID <- unlist(scopusID)
	doi <- lapply(records, XML::xpathSApply, "./prism:doi", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/")) ## handle potentially missing doi nodes
	doi[sapply(doi, is.list)] <- NA
	doi <- unlist(doi)
	pmid <- lapply(records, XML::xpathSApply, "./cto:pubmed-id", XML::xmlValue, namespaces = "cto") ## handle potentially missing pmid nodes: returns a list with the node value if the node is present and an empty list if the node is missing
	pmid[sapply(pmid, is.list)] <- NA ## find the empty lists in pmid and set them to NA
	pmid <- unlist(pmid) ## turn the pmid list into a vector
	authLast <- lapply(records, XML::xpathSApply, ".//cto:surname", XML::xmlValue, namespaces = "cto") ## grab the surname and initials for each author in each record, then paste them together 
	authLast[sapply(authLast, is.list)] <- NA
	authInit <- lapply(records, XML::xpathSApply, ".//cto:initials", XML::xmlValue, namespaces = "cto")
	authInit[sapply(authInit, is.list)] <- NA
	authors <- mapply(paste, authLast, authInit, collapse = "|")
	authors <- sapply(strsplit(authors, "|", fixed = TRUE), unique) ## remove the duplicate author listings
	authors <- sapply(authors, paste, collapse = "|")
	affiliations <- lapply(records, XML::xpathSApply, ".//cto:affilname", XML::xmlValue, namespaces = "cto") ## handle multiple affiliation names
	affiliations[sapply(affiliations, is.list)] <- NA
	affiliations <- sapply(affiliations, paste, collapse = "|")
	affiliations <- sapply(strsplit(affiliations, "|", fixed = TRUE), unique) ## remove the duplicate affiliation listings
	affiliations <- sapply(affiliations, paste, collapse = "|")
	countries <- lapply(records, XML::xpathSApply, ".//cto:affiliation-country", XML::xmlValue, namespaces = "cto")
	countries[sapply(countries, is.list)] <- NA
	countries <- sapply(countries, paste, collapse = "|")
	countries <- sapply(strsplit(countries, "|", fixed = TRUE), unique) ## remove the duplicate country listings
	countries <- sapply(countries, paste, collapse = "|") 
	year <- lapply(records, XML::xpathSApply, "./prism:coverDate", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/"))
	year[sapply(year, is.list)] <- NA
	year <- unlist(year)
	year <- gsub("\\-..", "", year) ## extract only year from coverDate string (e.g. extract "2015" from "2015-01-01")
	articletitle <- lapply(records, XML::xpathSApply, "./dc:title", XML::xmlValue, namespaces = c(dc = "http://purl.org/dc/elements/1.1/"))
	articletitle[sapply(articletitle, is.list)] <- NA
	articletitle <- unlist(articletitle)
	journal <- lapply(records, XML::xpathSApply, "./prism:publicationName", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/")) ## handle potentially missing issue nodes
	journal[sapply(journal, is.list)] <- NA
	journal <- unlist(journal)
	volume <- lapply(records, XML::xpathSApply, "./prism:volume", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/")) ## handle potentially missing issue nodes
	volume[sapply(volume, is.list)] <- NA
	volume <- unlist(volume)
	issue <- lapply(records, XML::xpathSApply, "./prism:issueIdentifier", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/")) ## handle potentially missing issue nodes
	issue[sapply(issue, is.list)] <- NA
	issue <- unlist(issue)
	pages <- lapply(records, XML::xpathSApply, "./prism:pageRange", XML::xmlValue, namespaces = c(prism = "http://prismstandard.org/namespaces/basic/2.0/")) ## handle potentially missing issue nodes
	pages[sapply(pages, is.list)] <- NA
	pages <- unlist(pages)
	abstract <- lapply(records, XML::xpathSApply, "./dc:description", XML::xmlValue, namespaces = c(dc = "http://purl.org/dc/elements/1.1/")) ## handle potentially missing abstract nodes
	abstract[sapply(abstract, is.list)] <- NA
	abstract <- unlist(abstract)
	keywords <- lapply(records, XML::xpathSApply, "./cto:authkeywords", XML::xmlValue, namespaces = "cto")
	keywords[sapply(keywords, is.list)] <- NA
	keywords <- unlist(keywords)
	keywords <- gsub(" | ", "|", keywords, fixed = TRUE)
	ptype <- lapply(records, XML::xpathSApply, "./cto:subtypeDescription", XML::xmlValue, namespaces = "cto")
	ptype[sapply(ptype, is.list)] <- NA
	ptype <- unlist(ptype)
	timescited <- lapply(records, XML::xpathSApply, "./cto:citedby-count", XML::xmlValue, namespaces = "cto")
	timescited[sapply(timescited, is.list)] <- NA
	timescited <- unlist(timescited)
	theDF <- data.frame(scopusID, doi, pmid, authors, affiliations, countries, year, articletitle, journal, volume, issue, pages, keywords, abstract, ptype, timescited, stringsAsFactors = FALSE)
	return(theDF)
}
```

```{r, fetch using title only}
#this function downloads citation data from scopus
FetchSpT <- function(genus, species, APIkey) {
  library(rscopus)
  library(rlang)
  library(taxize)
  if (is_missing(APIkey)) {
    stop("You need to register for an API key on Scopus.") #stops the function from running
  }
  search <- scopus_search(query = paste0("TITLE(\"",genus," ",species,"\") AND DOCTYPE(ar OR re)"),
                          api_key = paste0(APIkey),
                          verbose = TRUE)
  searchdf <- entries_to_citation_df(search$entries)
  return(searchdf)
}
```

```{r, fetch using title+abs+key}
#this function downloads citation data from scopus
FetchSpTAK <- function(genus, species, APIkey) {
  library(rscopus)
  library(rlang)
  if (is_missing(APIkey)) {
    stop("You need to register for an API key on Scopus.") #stops the function from running
  }
  search <- scopus_search(query = paste0("TITLE-ABS-KEY(\"",genus," ",species,"\") AND DOCTYPE(ar OR re)"),
                          api_key = paste0(APIkey),
                          verbose = TRUE)
  searchdf <- entries_to_citation_df(search$entries)
  return(searchdf)
}
```

```{r, loop test}
#this function downloads citation data from scopus
FetchSpTAKloop <- function(genus, species, APIkey) {
  library(rscopus)
  library(rlang)
  if (is_missing(APIkey)) {
    stop("You need to register for an API key on Scopus.") #stops the function from running
  }
  search <- scopus_search(query = paste0("TITLE-ABS-KEY(\"",genus," ",species,"\") AND DOCTYPE(ar OR re)"),
                          api_key = paste0(APIkey),
                          verbose = TRUE,
                          max_count = 3000,
                          start = 1,
                          wait_time = 1)
  while (search > 3000) { #loop
    print("fetching more records...")
    search <- scopus_search(query = paste0("TITLE-ABS-KEY(\"",genus," ",species,"\") AND DOCTYPE(ar OR re)"),
                          api_key = paste0(APIkey),
                          verbose = TRUE,
                          max_count = 3000,
                          start = length(search) + 1,
                          wait_time = 1)
  }
  searchdf <- entries_to_citation_df(search$entries)
  return(searchdf)
}
```

```{r, total publications}
#this function calculates the total number of publications
TotalPub <- function(data) {
  total <- nrow(data) #counts the number of publications by summing the records
  return(total)
}
```

```{r, total citations}
#this function calculates the total number of citations
TotalCite <- function(data) {
  data$citations <- as.numeric(data$citations) #changes the class to numerical for calculation
  total <- sum(data$citations) #sums all of the citations
  return(total)
}
```

```{r, total journals}
#this function calculates the total number of journals
TotalJournals <- function(data) {
  filter <- unique(data$journal) #filters data to pick out each journal
  total <- length(filter) #counts the number of journals
  return(total)
}
```

```{r, article:review ratio}
#this function calculates the ratio of articles:rerviews
Ratio <- function(data) {
  Article <- sum(data$description == "Article") #calculates the number of articles
  Review <- sum(data$description == "Review") #calculates the number of reviews
  ArticleRatio <- Article/(Article+Review)*100 #percentage of articles
  ReviewRatio <- Review/(Article+Review)*100 #percentage of reviews
  Ratio <- paste(ArticleRatio, ":", ReviewRatio) #prints ratio
  cat(Article, "articles", "\n",
      Review, "reviews", "\n",
      "Article : Review =", Ratio)
  return(Ratio)
}
```

```{r, species h-index}
#this function calculates the h-index
Hindex <- function(data) {
  data$citations <- as.numeric(data$citations) #change class from factor to numeric
  sorteddf <- sort(data$citations, decreasing = TRUE) #sort in descending order
  Hindex <- 0   #computes h-index
  for(i in 1:length(sorteddf)) {
    if (sorteddf[i] > Hindex) {
      Hindex <- Hindex + 1
    }
  }
  return(Hindex)
}
```

```{r, years of publication}
#this function canculates the number of years since the first publication
YearsPublishing <- function(data) {
  data$year <- as.numeric(substr(data$cover_date, 1, 4))
  as.numeric(substr(Sys.Date(), 1, 4)) - min(data$year) #time since first publication in years
  years_publishing <- as.numeric(substr(Sys.Date(), 1, 4)) - min(data$year) #time since first publication in years
  return(years_publishing)
}
```

```{r, m-index}
#this function calculates the m-index
#it does not requires the calculation of the h-index beforehand
Mindex <- function(data) {
  data$citations <- as.numeric(data$citations) #change class from factor to numeric
  sorteddf <- sort(data$citations, decreasing = TRUE) #sort in descending order
  Hindex <- 0   #computes h-index
  for(i in 1:length(sorteddf)) {
    if (sorteddf[i] > Hindex) {
      Hindex <- Hindex + 1
    }
  }
  data$year <- as.numeric(substr(data$cover_date, 1, 4))
  years_publishing <- as.numeric(substr(Sys.Date(), 1, 4)) - min(data$year) #time since first publication in years
  Mindex <- Hindex/years_publishing
  return(Mindex)
}
```

```{r, i10 index}
#this function calculates the i10 index
#i10 index counts all of the publications with 10+ citations
i10 <- function(data) {
  data$citations <- as.numeric(data$citations) #changes class from factor to numeric
  sorteddf <- sort(data$citations, decreasing = TRUE) #sorts in descending order
  i10 <- sum(sorteddf>=10) #counts the publications with 10 or more citations
  return(i10)
}
```

```{r, h5 index}
#this function calculates the h-index of the past 5 years
H5 <- function(data) { #last 5 years from the current date
  current_date <- as.numeric(substr(Sys.Date(), 1, 4)) #current year
  #The easiest thing to do is to convert it into POSIXlt and subtract 5 from the years slot.
  d <- as.POSIXlt(Sys.Date())
  d$year <- d$year-5
  as.Date(d)
  return(HAfterdate(data, d))
}
```

```{r, h-index with a given date}
#this function calculates the h-index using a given date up till the newest record
HAfterdate <- function(data, date) {
  library(dplyr)
  data$cover_date <- as.Date(data$cover_date, format = "%Y-%m-%d") #change format of the date from factor to date
  subsetdata <- filter(data, cover_date > as.Date(date) )
  HAfterdate <- Hindex(subsetdata)
  return(HAfterdate)
}
```

```{r, shows main indices}
#this function returns a summary of all of the indices
Allindices <- function(data) {
  combine <- data.frame(TotalPub(data), TotalCite(data), TotalJournals(data),
                        YearsPublishing(data), Hindex(data), Mindex(data), i10(data), H5(data))
  colnames(combine) <- c("publications", "citations", "journals",
                         "years_publishing", "h", "m", "i10", "h5")
  cat("", TotalPub(data), "publications", "\n",
      TotalCite(data), "citations", "\n",
      TotalJournals(data), "journals", "\n",
      YearsPublishing(data), "years of publishing", "\n",
      "h:", Hindex(data), "\n",
      "m:", Mindex(data), "\n",
      "i10:", i10(data), "\n",
      "h5:", H5(data))
  return(combine)
}
```

**Tests**

```{r, rscopus}
#some tests
tiger <- FetchSpT("Panthera", "tigris") #shows warning
tiger <- FetchSpTAK("Panthera", "tigris") #shows warning
fox <- FetchSpT("Vulpes", "vulpes", "442b9048417ef20cf680a0ae26ee4d86") #1133 results
foxTAK <- FetchSpTAK("Vulpes", "vulpes", "442b9048417ef20cf680a0ae26ee4d86") #3324 results
drosophila <- FetchSpTAK("Drosophila", "montana", "442b9048417ef20cf680a0ae26ee4d86") #93 results
drosophila2 <- FetchSpTAK("drosophila", "melanogaster", "442b9048417ef20cf680a0ae26ee4d86") #60904 results, error at 23%
drosophila3 <- FetchSpTAK("Mus", "musculus", "442b9048417ef20cf680a0ae26ee4d86") 
cat <- FetchSpTAKloop("felis", "catus", "442b9048417ef20cf680a0ae26ee4d86") #17622 results, error at 26%

FetchSpT("catus", "catus", "442b9048417ef20cf680a0ae26ee4d86")

searchCount("Mus", "musculus", "442b9048417ef20cf680a0ae26ee4d86") #34252 count

Pubxdf <- TotalPub(xdf) #929
Citexdf <- TotalCite(xdf) #16170
Journalxdf <- TotalJournals(xdf) #299
Ratio(xdf) #890:39, 95.8019375672766:4.19806243272336
Hxdf <- Hindex(tiger) #53
yearsxdf <- YearsPublishing(xdf) #139
Mxdf <- Mindex(xdf) #0.381294964028777
i10xdf <- i10(xdf) #467
h5xdf <- H5(xdf) #15
datexdf <- HAfterdate(xdf, "2000-01-01") #47
xdfALL <- Allindices(xdf)

```

```{r}
#test with dummy data
dummy1 <- data.frame("citations" = c(12, 5, 0, 21, 3, 4, 9, 17, 1, 10,
                                     10, 1, 0, 16, 26, 24, 15, 34, 31, 29),
                     "cover_date" = c(2000, 2015, 1986, 1994, 1972, 2002, 2006, 2019, 2016, 2006,
                                      1997, 1989, 2007, 2010, 2013, 2020, 2017, 1994, 1999, 2016))
Hindex(dummy1) #10
YearsPublishing(dummy1) #48
Mindex(dummy1) #0.2083333
i10(dummy1) #12


#total publications, total citations, total journals, reviews/journals percentage
```

```{r, Losia's h-index code}
      
citations <- c(0) #0
citations <- c(1) #1
citations <- c(0,0,0,0,0,0) #0
citations <- c(0,0,1,0,0,0) #1
citations <- c(2,3,0,5,6,0) #3
citations <- c(0,1,0,5,6,0) #2
citations <- c(4,4,4,1,4,0) #4
citations <- c(5,5,5,5,5,5) #5
citations <- c(6,6,6,6,6,6) #6
citations <- c(6,6,6,6,6,5) #5
citations <- c(6,6,6,9,9,9) #5
citations <- xdf$citations #53?

citations <- sort(citations, decreasing = TRUE)

mycount <- 0   
for(i in 1:length(citations)) {
  if (citations[i] > mycount) {
    mycount <- mycount + 1
  }
}
print(paste("h: ", mycount))

#see: https://publons.com/blog/5-things-the-h-index-cant-tell-you/
#also: m (Hirsch, 2005). M measures the slope or rate of increase of the H-index over time and is, in my view, a greatly underappreciated measure.   To calculate the m-value, take the researchers H-index and divide by the number of years since their first publication.
#H5Y factor. It is the H factor, but calculated only on citations received in the past five years. 

#i10 - check it out?
```
